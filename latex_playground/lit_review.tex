\documentclass{article}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}  % For inserting images
\usepackage{caption}   % For better caption formatting
\usepackage{float}     % To control figure placement
\usepackage{natbib}  % Recommended for author-year citations
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{todonotes}


\title{Literature Review}
\author{Eduardo Oliveira}
\date{\today}

\begin{document}

\maketitle

\listoftodos


\section{Enhancing Reproducibility in Scientific Research Through Open Science and Decentralized Technologies}

\subsection{The Imperative of Reproducibility in Scientific Research Science}

Science as a systematic and empirical pursuit of knowledge, fundamentally relies on the ability of researchers to verify and build upon the findings of their predecessors and peers. At the core of this process lies the concept of reproducibility, which encompasses both the capacity for others to obtain consistent results using the same data and methods, and the ability to achieve similar findings when new data is collected through the same experimental design \cite{pellizzari_reproducibility_2017, committee_2019}. A significant concern has emerged within the scientific community regarding the difficulty of reproducing the results of numerous published scientific studies across a wide spectrum of disciplines. This phenomenon, frequently referred to as the "reproducibility crisis", has shaken the foundations of scientific inquiry, leading to a growing lack of trust in research findings \cite{baker2016reproducibility}. The concerningly high rates of non-reproducible research, with studies suggesting an average failure rate of 50\%, indicate a systemic issue that extends beyond isolated cases of flawed methodology or misconduct \cite{branch_reproducibility_2019}. To provide context on the financial impact of low reproducibility rates in the life sciences, estimated annual losses in the United States alone exceed \$28 billion, primarily attributed to research that fails to meet reproducibility standards \cite{freedman2015economics}.


\subsection{Challenges to Scientific Integrity}

The consequences of the reproducibility crisis extends beyond the academia to affect public trust in science, slow down the translation of research into practical applications, and potentially lead to the misallocation of substantial resources and the implementation of misinformed policies based on unreliable findings. The inability to reproduce preclinical research, for example, can significantly delay the development of therapies that are live saving, increase the pressure on already strained research budgets, and drive up the costs associated with drug development. The societal impacts are also significant, with misdirected effort, funding, and policies potentially being implemented based on research that cannot be validated \cite{freedman2015economics}.

Several interconnected factors contribute to this crisis, spanning issues within the publication system to the prevalence of questionable research practices and the inherent complexities encountered in certain scientific disciplines. Journals often exhibit a publication bias, preferentially publishing novel and positive results while overlooking negative findings or replication studies \cite{ioannidis2005most}. This creates a skewed representation of the scientific landscape and can lead to the neglect of important information about what does not work \cite{collins_policy_2014}. Furthermore, researchers may engage in questionable research practices, such as p-hacking (manipulating data to achieve statistical significance) and HARKing (hypothesizing after results are known), which can distort results and make replication exceedingly challenging. Inadequate statistical methods, including the use of suboptimal analyses, can also lead to erroneous conclusions, further hindering the replication process. A significant contributing factor is the lack of data sharing among researchers; when data and methods are not openly accessible, the ability of others to verify and replicate the work is severely limited \cite{munafo_manifesto_2017}.

The intense pressure to publish, often described by the expression "publish or perish," can incentivize researchers to prioritize the quantity of publications over their quality, potentially leading to rushed and less rigorous research. Incentive structures within universities may inadvertently reward the mere act of publication in prestigious journals, sometimes at the expense of methodological rigor and the pursuit of accurate and reproducible findings. This competitive environment can implicitly or explicitly encourage the use of questionable research practices to achieve publication, such as selectively reporting parts of datasets or trying different analytical approaches until the desired outcome is obtained \cite{david_robert_grimes_modelling_2018}.

The reproducibility crisis in science also reveals a strong connection between data management practices and the ability to replicate experimental results. Transparent and accessible data are essential for verifying findings and ensuring their reliability across disciplines. Insufficient metadata, unavailability of raw data, and incomplete methodological reporting are major contributors to irreproducibility. Without proper documentation and sharing protocols, researchers face significant barriers in reusing or validating published results \cite{samuel_understanding_2021}.

\subsection{A Paradigm Shift Towards Transparency and Collaboration}

In response to concerns about the reproducibility and reliability of scientific production, a movement emerged advocating for a fundamental transformation in how knowledge is generated and disseminated, emphasizing transparency, accessibility, and collaboration within the scientific community and with the broader public. Although the ideals of openness and sharing have long been embedded in scientific practice, the Open Science movement gained momentum with the advent of the internet and the more interactive capabilities made available by the Web 2.0 \cite{thibault_open_2023}.

\subsection{Open Science Principles as Solutions to the Reproducibility Crisis}

The Open Science practices are designed to confront reproducibility issues by promoting greater transparency, accessibility, and collaboration in scientific research. Among these practices, five core principles stand out: Open Data, Open Materials, Open Access, Preregistration, and Open Analysis. These principles address systemic issues that undermine the credibility and reliability of scientific outputs and seek to realign research practices with the foundational values of openness and verifiability \cite{van_dijk_open_2021}.

\begin{table}[ht]
    \centering
    \caption{The Five Principles of Open Science \cite{van_dijk_open_2021}}
    \label{tab:open_science_principles}
    \begin{tabular}{|l|p{11cm}|}
        \hline
        \textbf{Principle} & \textbf{Description}                                                                                                                                                 \\
        \hline
        Open Data          & Making research data freely available for others to inspect, reuse, and build upon, supporting transparency and reproducibility.                                     \\
        \hline
        Open Analysis      & Sharing code, workflows, and analysis scripts used in the study to allow others to verify and replicate the results.                                                 \\
        \hline
        Open Materials     & Providing full access to the materials, tools, and instruments used in the research, such as surveys, interventions, protocols or software.                          \\
        \hline
        Preregistration    & Publicly registering study designs, hypotheses, and analysis plans before data collection to prevent selective reporting and increase research integrity.            \\
        \hline
        Open Access        & Ensuring that research outputs, including publications, are freely accessible to all, removing barriers imposed by paywalls, subscritpions or restrictive licensing. \\
        \hline
    \end{tabular}
\end{table}

A central element of this framework is the commitment to Open Data, which calls for unrestricted access to raw research data and associated metadata. This principle directly addresses the lack of transparency that often impedes reproducibility by ensuring that the empirical foundation of research is available for validation, reinterpretation, and reuse. Open Data repositories serve a critical role in this ecosystem by preserving datasets in standardized formats, maintaining provenance metadata, and enabling persistent access. Provenance information about the origin, context, and transformations applied to the data is particularly important, as it supports reproducibility by providing a traceable record of how datasets were collected, processed, and interpreted. Without these metadata standards and traceability mechanisms, shared data risk becoming uninterpretable or misleading when repurposed \cite{learn_2017, burgelman_open_2019}.

Linked to Open Data is the principle of Open Materials, which involves making the research components such as experimental protocols, instructions and interventions. Open Materials ensure that researchers seeking to replicate a study or extend its methodology have access to the same inputs and tools used in the original work. Depositing these materials in domain-specific repositories and documenting them with clear metadata and provenance records enhances both transparency and usability \cite{van_dijk_open_2021}.

Open Access complements these practices by addressing the dissemination of research outputs. It entails making peer-reviewed publications freely available without subscription or payment barriers. Open Access expands the reach and impact of scientific knowledge, enabling researchers from under-resourced institutions and disciplines to participate in scholarly discourse and replication efforts. In conjunction with preprints—versions of manuscripts shared prior to peer review—Open Access accelerates the circulation of ideas and allows the broader community to scrutinize findings earlier in the research lifecycle. This early-stage visibility invites broader feedback and can help identify methodological flaws or inconsistencies that might otherwise go unnoticed until post-publication \cite{van_dijk_open_2021}.

To strengthen methodological transparency, Open Science also promotes Preregistration, which involves submitting a time-stamped outline of the research questions, hypotheses, and study design prior to data analysis. The adoption of preregistration discourages questionable research practices such as HARKing (Hypothesizing After the Results are Known) and p-hacking, thereby increasing transparency and reducing publication bias. This enhances the credibility of findings throughout the experimental process. Preregistered reports can be submitted to dedicated registries, assigned unique identifiers, and tracked by provenance systems that ensure the integrity and traceability of the research workflow \cite{van_dijk_open_2021}.

Finally, Open Analysis entails sharing the code and computational workflows used in data processing and statistical inference. By making analysis pipelines available, researchers allow others to reproduce exact outputs from shared data, supporting both validation and reuse. Integration with containerization tools, version control systems, and computational notebooks strengthens this principle, enabling complete provenance tracking of computational environments and decisions \cite{van_dijk_open_2021}.

Finally, Open Analysis involves the disclosure of code and computational workflows employed in data processing and statistical inference. By making analysis pipelines accessible, researchers enable others to reproduce the exact outputs from shared datasets, thereby facilitating both validation and reuse. The adoption of containerization tools, version control systems, and computational notebooks further reinforces this principle by enabling comprehensive provenance tracking of computational environments and analytical decisions \cite{van_dijk_open_2021, samuel_understanding_2021}.

Together, the five principles of Open Science—Open Data, Open Materials, Open Analysis, Preregistration, and Open Access form a cohesive approach to improving the reliability and transparency of scientific research. By promoting the use of open repositories, standardized metadata, and accessible workflows, these practices reshape how knowledge is produced and shared, fostering a more trustworthy and collaborative research environment.

\subsection{Current Initiatives and Standards for Enhancing Research Reproducibility}

\subsection{Key Initiatives in Open Science and Research Data Management}

The growing emphasis on transparency, reproducibility, and collaboration in scientific research has led to the emergence of several influential initiatives that support the implementation of Open Science and effective Research Data Management (RDM). These initiatives provide frameworks, tools, and community-driven guidelines that help researchers and institutions manage data more responsibly, ensuring that research outputs are not only preserved but also accessible and reusable. By fostering interoperability, encouraging FAIR (Findable, Accessible, Interoperable, and Reusable) data practices, and promoting a culture of openness, these efforts contribute to a more trustworthy and efficient research ecosystem. This section discusses a selection of leading initiatives spanning international collaborations, policy frameworks, and infrastructural developments that collectively shape the evolving landscape of Open Science and RDM.

\subsection{Key Initiatives in Research Data Management and Open Science}

\subsection{Leveraging European Research Data (LEARN)}
The LEARN Toolkit (Leveraging European Research Data) was developed to assist research institutions in implementing effective Research Data Management (RDM) policies and practices. Grounded in the recommendations of the LERU (League of European Research Universities) Roadmap for Research Data, the Toolkit offers guidance on institutional policy development, advocacy, training, infrastructure, and best practices. It emphasizes the strategic role of data management planning and encourages institutions to embed RDM into the research lifecycle. By providing a series of model policies, case studies, and checklists, LEARN promotes a culture of data stewardship aligned with the principles of FAIR data (Findable, Accessible, Interoperable, and Reusable), contributing to the broader objectives of Open Science \cite{learn_2017}.

\subsection{FAIR Guding Principles}
The FAIR Guiding Principles represents a cornerstone of responsible data stewardship in the context of Open Science. These principles aim to improve the infrastructure supporting the reuse of scholarly data. By encouraging data producers to make their outputs Findable, Accessible, Interoperable, and Reusable, FAIR fosters machine-readability, long-term preservation, and seamless data integration across platforms and disciplines. Although not inherently open, FAIR complements Open Science by providing the technical and semantic standards necessary for data sharing and reuse. Adoption of FAIR principles by research funders, repositories, and institutions has significantly influenced data policies across scientific communities and reinforced efforts toward more transparent and collaborative research practices \cite{wilkinson_fair_2016}.


\subsection{GO FAIR}
The GO FAIR initiative builds on the momentum of the FAIR principles, functioning as a bottom-up, stakeholder-driven movement to implement FAIR data stewardship globally. It encourages the development of implementation networks—collaborative groups that share expertise and develop domain-specific solutions for achieving FAIR data practices. GO FAIR’s focus extends to governance, education, and infrastructure, aiming to create a distributed ecosystem that facilitates the reuse of scientific data. By promoting interoperability standards and cultural change across the scientific community, GO FAIR advances Open Science by ensuring that data outputs can be seamlessly discovered, accessed, and reused across institutional and national boundaries \cite{henning_go_2019}.


\subsection{Research Data Alliance (RDA)}
The Research Data Alliance (RDA) is a global community-driven initiative that brings together data practitioners, technologists, and policymakers to build the social and technical infrastructure necessary for open data sharing across disciplines. Founded in 2013, RDA operates through working groups and interest groups that develop recommendations, standards, and best practices for data interoperability and stewardship. The RDA fosters international cooperation and bridges disciplinary gaps by aligning data governance, metadata standards, and infrastructure development. Its outputs support the implementation of Open Science by ensuring that research data is not only preserved but also rendered useful and actionable across diverse research contexts \cite{berman_research_2020}.

\subsection{Committee on Data of the International Science Council(CODATA)}
CODATA is an international organization committed to advancing data science and improving the quality and accessibility of research data. It plays a vital role in the global Open Science ecosystem by supporting the development of data policies, fostering international collaboration, and providing strategic guidance on data governance. CODATA actively contributes to the advancement of the FAIR principles and supports initiatives that aim to make research data a reusable, sustainable, and equitable public good. Through its coordination efforts and engagement with global stakeholders, CODATA helps shape the infrastructures and norms that underpin responsible data sharing and Open Science \cite{codata_2024}.

\subsection{Open Access Infrastructure for Research in Europe (OpenAIRE)}
OpenAIRE represents a pan-European initiative designed to support the open dissemination and reuse of research outputs. Originating as a response to the European Commission's Open Access policies, OpenAIRE has developed into a robust infrastructure that aggregates metadata and full-text content from a wide array of data providers, including institutional repositories, data archives, and scholarly journals. By facilitating interlinking between publications, datasets, software, and project information, OpenAIRE enhances the discoverability and interoperability of research products across disciplines. Its suite of services, such as the OpenAIRE Graph and Research Community Dashboards, provides tools for compliance monitoring, impact assessment, and reproducibility tracking. Furthermore, OpenAIRE actively contributes to policy development and technical alignment in the global Open Science ecosystem, advocating for standardized metadata schemas and persistent identifiers. Through its alignment with FAIR principles and support for the European Open Science Cloud (EOSC), OpenAIRE plays a foundational role in shaping a transparent, interconnected, and researcher-centric data landscape \cite{rettberg_openaire_2012}.

\subsection{DataCite}

DataCite is a global non-profit organization that plays a foundational role in the research data ecosystem by providing persistent identifiers—most notably Digital Object Identifiers (DOIs)—for datasets and other research outputs. Founded to support data citation practices, DataCite promotes the discoverability, accessibility, and reuse of research data by ensuring that data can be persistently linked to scholarly publications and contributors. It collaborates with data centers, publishers, and repositories to establish metadata standards that facilitate interoperability across infrastructures. Through services such as DOI registration, metadata management, and citation tracking, DataCite actively contributes to the implementation of the FAIR principles and strengthens the overall architecture of Open Science and Research Data Management worldwide \cite{brase_datacite_2009}.

\subsection{Nelson Memo - Office of Science and Technology Policy (OSTP)}

In 2022, the White House Office of Science and Technology Policy (OSTP) issued a directive known as the “Nelson Memo,” which requires that all federally funded research publications and associated data be made immediately and freely available to the public by December 31, 2025. This policy marks a pivotal shift in U.S. open access strategy by eliminating embargo periods and strengthening mandates for data transparency. Building on previous open science policies, the Nelson Memo seeks to ensure equitable access to publicly funded knowledge, drive reproducibility, and accelerate scientific progress through a national commitment to openness and accountability \cite{nelson_2023}.


\begin{table}[H]
    \centering
    \caption{Comparison of Open Science Related Initiatives}
    \label{tab:initiative_comparison}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|p{3.5cm}|p{4cm}|p{5cm}|p{5cm}|}
            \hline
            \textbf{Initiative}                & \textbf{Coverage}                           & \textbf{Key Outputs}                                                        & \textbf{Contribution to Open Science}                                                        \\
            \hline
            LEARN                              & Institutional; Europe (globally applicable) & RDM policy toolkit, model policies, case studies                            & Strengthens institutional capacity for implementing FAIR and Open Data policies              \\
            \hline
            FAIR Guiding Principles            & European Union                              & FAIR assessment tools, training materials, recommendations                  & Embeds FAIR principles into research workflows and infrastructures                           \\
            \hline
            GO FAIR                            & Global                                      & FAIRification framework, implementation networks, training modules          & Operationalizes FAIR principles through community-driven practices                           \\
            \hline
            EOSC (European Open Science Cloud) & Pan-European Infrastructure                 & EOSC Portal, service registry, metadata standards                           & Provides federated infrastructure to enable Open Science practices across disciplines        \\
            \hline
            RDA (Research Data Alliance)       & Global                                      & Working group outputs, interoperability guidelines, standards               & Enhances technical and social infrastructure for global data sharing                         \\
            \hline
            CODATA                             & Global (UNESCO)                             & Policy frameworks, capacity-building initiatives, data science standards    & Supports Open Science through coordination of global data policy and governance              \\
            \hline
            OpenAIRE                           & European Union                              & Research Graph, repository integration tools, metadata guidelines           & Connects RDM and Open Access via aggregated infrastructure and metadata interoperability     \\
            \hline
            DataCite                           & Global                                      & DOI registration service, metadata schema, discovery APIs, DataCite Commons & Enables FAIR data by ensuring traceability, citation, and persistent access in open research \\
            \hline
            OSTP Nelson Memo                   & United States / Federal Policy              & Mandate immediate public access to federally funded research outputs        & Shapes U.S. policy landscape for Open Access and data sharing by 2026                        \\
            \hline
        \end{tabular}%
    }
\end{table}


\section*{Decentralized Applications in Support of Open Science and Reproducibility}

The limitations of traditional research data management systems have sparked growing interest in alternative models. Decentralized technologies, particularly blockchain, have gained increasing recognition for their ability to enhance transparency, accountability, and trust across various domains, including scientific research. Their potential to address long-standing inefficiencies and structural shortcomings within the research ecosystem has attracted significant attention from the academic community.

Blockchain has evolved into a broader paradigm of distributed ledger technology, collectively maintained by a network of nodes. Through immutability and consensus-based validation, it ensures the integrity of recorded data. These foundational features offer a technological infrastructure for verifying the authenticity, provenance, and persistence of digital records, features that align closely with Open Science objectives and the FAIR principles (Findable, Accessible, Interoperable, and Reusable). In an era of data-intensive research and multi-stakeholder collaboration, such assurances are critical for enabling reproducibility, facilitating the auditability of research processes, and ensuring reliable attribution of intellectual contributions.

Decentralized solutions introduce a novel approach to scientific data governance. This paradigm shift directly supports key principles of Open Science such as openness, inclusivity, reproducibility, and collaboration by embedding accountability and traceability into the technical fabric of research infrastructures.

In this context, decentralized applications function as strategic enablers of both cultural and procedural transformation in science. They offer pathways to reconfigure incentive structures, reduce access barriers, and reinforce the reproducibility and credibility of scientific outputs. The remainder of this section explores the current state of such applications, their underlying architectures, and the roles they play in advancing Open Science and addressing reproducibility challenges.

Blockchain technologies have increasingly drawn scholarly attention for their potential to transform academic publishing and research dissemination. An early synthesis of this trajectory is provided by \cite{lawlor_overview_2018}, who captured discussions from the 2018 NFAIS conference on blockchain in scholarly communication. The article emphasized blockchain’s capacity to reshape research workflows, peer review, and intellectual property management, spotlighting pilot initiatives such as ARTiFACTS and Knowbella Tech. These efforts highlighted the technology’s promise for enhancing provenance tracking and decentralizing research funding. However, despite visible enthusiasm for secure, transparent, and decentralized scholarly infrastructures, the article concluded that broader adoption hinges on increasing awareness and achieving a more nuanced understanding of blockchain’s capabilities.

Echoing this vision of decentralization, \cite{holmen_blockchain_2018} explores how blockchain might disrupt traditional academic publishing workflows by unbundling them and empowering content creators with more equitable systems of recognition and compensation. Through token-based platforms like Steem, BAT, and LBRY, the article illustrates alternatives to centralized content discovery systems that could provide direct value to researchers. It argues that designing systems centered on discoverability and monetization may help build more effective and transparent publishing ecosystems. Nevertheless, it acknowledges that realizing these benefits would require a fundamental rethinking of current revenue models.

In a similar fashion, \cite{kochalko_making_2019} frames blockchain as a vehicle for fulfilling Eugene Garfield’s vision of recognizing a broader spectrum of scholarly contributions. Using ARTiFACTS as a case study, the article describes how blockchain can secure provenance and attribution even during the early stages of research. This capability opens the door to formal recognition for pre-publication outputs and thereby alters existing academic incentive structures. It presents a long-term outlook in which blockchain infrastructure supports not only open science principles but also new academic reputation systems.

A broader perspective is provided by \cite{van_rossum_blockchain_2018}, who surveys blockchain’s potential to address inefficiencies and credibility deficits in science. The article discusses its applications in peer review, reproducibility, and research funding via cryptocurrency-based mechanisms. Concepts such as decentralized repositories, micropayments, and digital rights management are introduced as possible innovations for academic publishing. While the potential to support transparency and equity is clear, the article cautions that overcoming entrenched institutional inertia remains a major hurdle to implementation.

Technical pathways toward integrating blockchain with existing systems are examined in \cite{gazis_blockchain_2022}, which proposes a blockchain-based cloud middleware to improve manuscript submission and peer review. Rather than disrupting current infrastructures, the framework emphasizes compatibility, aiming to enhance anonymity, reduce bias, and increase decentralization. Built with Java Spring and Ethereum, and tested on simulated data, the proposed system demonstrates viability but also exposes real-world challenges related to scalability and integration.

Offering a broader field overview, \cite{leible_review_2019} provides a systematic review of 60 blockchain-based projects, mapping their relevance to open science. The article identifies decentralization, immutability, and transparency as key alignments between blockchain and open science principles. Projects span areas such as reproducibility, secure sharing, and intellectual property management. Yet despite the enthusiasm, the article points out barriers like the lack of standardization, technical risks from smart contracts, and the difficulty of creating sustainable incentive models. It concludes that while blockchain is promising, a supportive ecosystem is required for its broader acceptance and functionality within scientific communities.

Practical experimentation with blockchain for peer review incentives is addressed by \cite{trovo_ants-review_2021}, which introduces Ants-Review, a protocol utilizing Ethereum smart contracts to reward high-quality anonymous reviews. The system uses ANTS tokens as bounties for peer reviews and integrates the AZTEC Protocol to maintain anonymity. Through a gamified evaluation process and plans for Decentralized Finance (DeFi) integration and DAO-based governance, the authors propose a robust and inclusive mechanism to promote fairer and more efficient scientific publishing.

The potential for blockchain’s potential applications in academia is examined \cite{kosmarski_blockchain_2020}, including its role in open data sharing, governance, and the transparent evaluation of scholarly outputs through immutable tracking of citations and usage metrics. While the outlook is optimistic, the study also notes ongoing challenges such as the complexity of user interfaces, unresolved legal questions, and the friction between decentralized technologies and established academic conventions.

Exploring newer technologies, \cite{putnings_non-fungible_2022} proposes non-fungible tokens (NFTs) as a means to restore ownership and value in scholarly communication. The article suggests pathways for integrating NFTs into university presses and submission platforms, under conditions of zero cost, autonomy, interoperability, and ease of use. Complementing this vision is the development of Open Lab \cite{shevchenko_open_2022}, a browser-based experiment platform that integrates with the Open Science Framework to simplify participant management and promote open collaboration, thus further supporting the ethos of transparent and reproducible research.

A more security-focused solution is proposed by \cite{zhou_open-pub_2021}, who introduce Open-Pub, a private Ethereum-based publishing system. Addressing the dual demands of privacy and transparency, it features a threshold identity-based group signature scheme for anonymous peer review and leverages IPFS for decentralized data storage. A token-based incentive structure for reviewers aims to strengthen community engagement. Although incentives for authors are less explored, the architecture reflects the growing demand for systems that reconcile openness with secure control in academic publishing.

Finally, \cite{lee_unblocking_2023} critiques the limitations of current academic recognition models, which prioritize publications and citations while overlooking essential but undervalued contributions. Proposing a blockchain-based token economy, the article advocates rewarding a wider range of academic activities—including peer review and committee work—with non-tradable, non-transferable tokens, offering a pathway to more holistic and inclusive systems of academic recognition.

\todo{Insert table - Causes and Impacts of the Reproducibility Crisis, see tables.tex}
\todo{Insert table: Summary of Reviewed Articles, see tables.tex}
\todo{Insert table: Cross-Cutting Themes and Blockchain Applications, see tables.tex}
\todo{Review captions and labels in tables}
\todo{Review sections and subsections}
\todo{Review Conclusion}


In conclusion, the body of research reviewed in this section illustrates how blockchain technology can serve as a foundational enabler not only for reforming scholarly publishing but also for advancing the broader goals of Open Science and addressing enduring concerns around reproducibility. The studies highlight a range of blockchain applications, from enhancing the transparency and accountability of peer review processes to enabling novel systems of contributor recognition and facilitating the secure management of digital research assets. These innovations are closely aligned with the principles of Open Science, including openness, accessibility, interoperability, and the traceability of scientific processes. Beyond the publishing lifecycle, blockchain offers mechanisms to strengthen research integrity by ensuring provenance, versioning, and tamper-proof audit trails for data and workflows, all of which are critical for improving reproducibility in scientific research. Nonetheless, the transition from conceptual promise to systemic adoption requires overcoming substantial barriers related to technological maturity, governance, interoperability with existing infrastructure, and user acceptance across diverse research communities. Moving forward, the path toward meaningful integration of blockchain into scholarly communication and Open Science will depend on collaborative, interdisciplinary efforts aimed at designing user-oriented solutions that address real-world research challenges. A pragmatic and critical perspective—one that embraces blockchain's potential while remaining attentive to its limitations—will be essential for harnessing the technology to build a more trustworthy, transparent, and inclusive research ecosystem.


\bibliographystyle{plain}
\bibliography{Bibliography.bib}


\end{document}


